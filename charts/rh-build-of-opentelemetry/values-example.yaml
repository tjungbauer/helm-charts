---
# Example values file for Red Hat Build of OpenTelemetry
# This demonstrates a complete production configuration

namespace:
  name: opentelemetry
  create: true
  display: "OpenTelemetry Observability"
  descr: "Production OpenTelemetry Stack"

#########################################################################################
# OPENTELEMETRY COLLECTOR - Production Configuration
#########################################################################################
collector:
  enabled: true
  name: otel-collector
  mode: deployment
  replicas: 3
  
  serviceAccount: otel-collector-sa
  managementState: managed
  
  resources:
    limits:
      cpu: 2
      memory: 4Gi
    requests:
      cpu: 500m
      memory: 1Gi
  
  nodeSelector:
    node-role.kubernetes.io/worker: ""
  
  tolerations:
    - key: node-role.kubernetes.io/infra
      operator: Exists
      effect: NoSchedule
  
  config:
    receivers:
      # OTLP receivers for traces, metrics, and logs
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
            cors:
              allowed_origins:
                - "https://*.example.com"
      
      # Prometheus receiver for scraping metrics
      prometheus:
        config:
          scrape_configs:
            - job_name: 'otel-collector'
              scrape_interval: 30s
              static_configs:
                - targets: ['0.0.0.0:8888']
      
      # Jaeger receiver (if migrating from Jaeger)
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
    
    processors:
      # Batch processor for better throughput
      batch:
        timeout: 10s
        send_batch_size: 1024
        send_batch_max_size: 2048
      
      # Memory limiter to prevent OOM
      memory_limiter:
        check_interval: 1s
        limit_mib: 3500
        spike_limit_mib: 512
      
      # Resource processor to add cluster attributes
      resource:
        attributes:
          - key: cluster.name
            value: production-cluster
            action: insert
          - key: k8s.cluster.name
            value: production-cluster
            action: insert
          - key: deployment.environment
            value: production
            action: insert
      
      # K8s attributes processor
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
    
    exporters:
      # Export traces to Tempo
      otlp/tempo:
        endpoint: tempo-gateway.tempo.svc.cluster.local:4317
        tls:
          insecure: false
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      
      # Export metrics to Prometheus
      prometheusremotewrite:
        endpoint: http://prometheus-server.monitoring.svc.cluster.local:9090/api/v1/write
        resource_to_telemetry_conversion:
          enabled: true
      
      # Export logs to Loki
      loki:
        endpoint: http://loki-gateway.loki.svc.cluster.local:3100/loki/api/v1/push
        labels:
          resource:
            k8s.namespace.name: "namespace"
            k8s.pod.name: "pod"
            k8s.container.name: "container"
      
      # Debug exporter for troubleshooting
      debug:
        verbosity: basic
        sampling_initial: 5
        sampling_thereafter: 200
    
    service:
      pipelines:
        # Traces pipeline
        traces:
          receivers: [otlp, jaeger]
          processors: [memory_limiter, k8sattributes, resource, batch]
          exporters: [otlp/tempo]
        
        # Metrics pipeline
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, resource, batch]
          exporters: [prometheusremotewrite]
        
        # Logs pipeline
        logs:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, resource, batch]
          exporters: [loki]
      
      telemetry:
        logs:
          level: info
        metrics:
          level: detailed
          address: 0.0.0.0:8888

#########################################################################################
# OPENTELEMETRY INSTRUMENTATION - Auto-instrumentation Configuration
#########################################################################################
instrumentation:
  enabled: true
  name: otel-instrumentation
  
  exporter:
    endpoint: http://otel-collector.opentelemetry.svc.cluster.local:4318
  
  propagators:
    - tracecontext
    - baggage
    - b3
    - jaeger
  
  sampler:
    type: parentbased_traceidratio
    argument: "0.25"  # 25% sampling rate
  
  # Java applications
  java:
    env:
      OTEL_EXPORTER_OTLP_TIMEOUT: "10000"
      OTEL_TRACES_EXPORTER: "otlp"
      OTEL_METRICS_EXPORTER: "otlp"
      OTEL_LOGS_EXPORTER: "otlp"
      OTEL_INSTRUMENTATION_COMMON_PEER_SERVICE_MAPPING: "redis=redis-service,postgresql=postgres-service"
  
  # Node.js applications
  nodejs:
    env:
      OTEL_EXPORTER_OTLP_TIMEOUT: "10000"
      OTEL_NODE_RESOURCE_DETECTORS: "env,host,os,serviceinstance"
  
  # Python applications
  python:
    env:
      OTEL_EXPORTER_OTLP_TIMEOUT: "10000"
      OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED: "true"
  
  # .NET applications
  dotnet:
    env:
      OTEL_EXPORTER_OTLP_TIMEOUT: "10000"
      OTEL_DOTNET_AUTO_TRACES_INSTRUMENTATION_ENABLED: "true"
      OTEL_DOTNET_AUTO_METRICS_INSTRUMENTATION_ENABLED: "true"

#########################################################################################
# TARGET ALLOCATOR - Prometheus Target Discovery
#########################################################################################
targetallocator:
  enabled: true
  name: otel-targetallocator
  replicas: 2
  
  serviceAccount: otel-targetallocator-sa
  
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi
  
  nodeSelector:
    node-role.kubernetes.io/worker: ""
  
  allocationStrategy: consistent-hashing
  
  prometheusCR:
    enabled: true
    scrapeInterval: 30s
    
    # Discover ServiceMonitors with specific labels
    serviceMonitorSelector:
      matchLabels:
        prometheus: kube-prometheus
    
    # Discover PodMonitors with specific labels
    podMonitorSelector:
      matchExpressions:
        - key: monitoring
          operator: In
          values:
            - enabled
            - true
  
  env:
    - name: OTEL_LOG_LEVEL
      value: "info"

